{
    "version": "https://jsonfeed.org/version/1",
    "title": "Oikiou's Blog • All posts by \"audio\" tag",
    "description": "Blog",
    "home_page_url": "https://www.oikiou.top",
    "items": [
        {
            "id": "https://www.oikiou.top/2024/2f262a28/",
            "url": "https://www.oikiou.top/2024/2f262a28/",
            "title": "音视频编解码",
            "date_published": "2024-07-21T08:23:05.000Z",
            "content_html": "<h1 id=\"视频显示原理\"><a href=\"#视频显示原理\" class=\"headerlink\" title=\"视频显示原理\"></a>视频显示原理</h1><p>视频是利用人眼视觉暂留的原理，通过播放一系列的图片，使人眼产生运动的感觉。单纯传输视频画面，视频量非常大，对现有的网络和存储来说是不可接受的。为了能够使视频便于传输和存储，人们发现视频有大量重复的信息，如果将重复信息在发送端去掉，在接收端恢复出来，这样就大大减少了视频数据的文件，因此有了各种视频压缩标准。</p>\n<h1 id=\"视频编码\"><a href=\"#视频编码\" class=\"headerlink\" title=\"视频编码\"></a>视频编码</h1><p><a href=\"https://www.cnblogs.com/yongdaimi/p/10676309.html\">参考 I帧、P帧、B帧、GOP、IDR 和PTS, DTS之间的关系</a><br><a href=\"https://blog.csdn.net/BigDream123/article/details/120329879\">视频编码基础：I帧、P帧 和 B 帧</a></p>\n<blockquote>\n<p>拓展知识：视频编码方式<br>H.26x（由ITU[国际电传视讯联盟]主导）<br>MPEG-x （由ISO[国际标准组织机构]下属的MPEG[运动图象专家组]开发）<br>VP-x （Google开发的开源视频编码格式）<br>AV1 （由Alliance for Open Media 开放媒体联盟，开发的一种开源视频编码格式）</p>\n<ul>\n<li><strong>MPEG-4</strong> 与H.264诞生了H.264&#x2F;AVC标准</li>\n<li><strong>H.264&#x2F;AVC</strong> H.264&#x2F;MPEG-4第十部分，或称AVC（Advanced Video Coding，高级视频编码）</li>\n<li><strong>H.265&#x2F;HEVC</strong> （High Efficiency Video Coding，简称HEVC）H.264的继任者，最高分辨率可达到8192×4320，HEVC被认为不仅提升图像质量，同时也能达到<strong>H.264&#x2F;MPEG-4</strong> AVC两倍之压缩率。唯一的潜在缺点是许可系统有点复杂。</li>\n<li><strong>H.266&#x2F;VVC</strong>  Versatile Video Coding H.266 的开发目的是超越 H.265。但仍然存在一些与前身类似的许可和版税问题需要解决。与 H.265 相比，在相同的感知质量下，H.266 能够将比特率降低 30% 至 50%。</li>\n<li><strong>VP9</strong> VP9 作为 VP8 的后继产品，主要竞争对手是 MPEG 的高效视频编码标准 HEVC。</li>\n<li><strong>AV1</strong> 旨在提供更高效的压缩和更好的视频质量。它免版税且开源，效率比 VP9 高 30%。</li>\n</ul>\n<p>对比：</p>\n<ol>\n<li>压缩效率：H.265和AV1相对于H.264、MPEG-2、MPEG-4和VP9具有更高的压缩效率，可以在相同的视频质量下减少更多的码率，从而减少带宽和存储需求。</li>\n<li>支持的分辨率和帧率：H.264、MPEG-2、MPEG-4和VP9支持较低的分辨率和帧率，而H.265和AV1支持更高的分辨率和帧率，例如4K和8K视频。</li>\n<li>编码延迟：H.265和AV1相对于H.264、MPEG-2、MPEG-4和VP9具有更高的编码延迟，这意味着它们需要更长的时间来编码视频，这可能会影响实时视频流应用。</li>\n<li><strong>编码复杂度</strong>：H.265和AV1相对于H.264、MPEG-2、MPEG-4和VP9具有更高的编码复杂度，这意味着需要更强的计算能力来进行编码。</li>\n<li>开放性：AV1是一种开放标准，由多个公司和组织共同开发，可以免费使用，而H.264、H.265、MPEG-2、MPEG-4和VP9则有专利和版权限制。<br>AV1和H.265的压缩比较高，所以编码较为复杂，延迟较高，计算量更高。</li>\n</ol>\n<p><strong>需要注意的是 MKV MP4 AVI 等等这些是音视频容器，和视频编码不是一个概念，视频容器是用来盛放各种视频流、音频流、字幕等等数据的“容器”。不同的容器里面可能包含着相同的视频流。</strong></p>\n</blockquote>\n<p>编码器将多张图像进行编码后生产成一段一段的 GOP ( Group of Pictures ) ， 解码器在播放时则是读取一段一段的 GOP 进行解码后读取画面再渲染显示。<br>GOP ( Group of Pictures) 是一组连续的画面，由一张 I 帧和数张 B &#x2F; P 帧组成，是视频图像编码器和解码器存取的基本单位，它的排列顺序将会一直重复到影像结束。</p>\n<ul>\n<li><strong>I 帧</strong>是内部编码帧（也称为关键帧）是一个完整的图像帧，它独立于其他帧存在。I帧不依赖于其他帧的信息即可独立解码，类似于静态图像，可以视为视频序列中的一个参考点。由于I帧包含了完整的图像信息，其压缩率相对较低，但在解码时最为简单，因为它不涉及对其他帧的依赖。</li>\n<li><strong>P 帧</strong>是前向预测帧（前向参考帧）依赖于前面的I帧或P帧来生成。P帧存储的是与前一帧相比图像的变化量，因此它的压缩效果通常比I帧更好。在解码P帧时，需要先解码它所依赖的I帧或P帧，然后根据这些信息来重建当前帧的画面。P帧的引入有效减少了时间维度上的冗余，提高了视频的压缩效率。</li>\n<li><strong>B 帧</strong>是双向内插帧（双向参考帧）需要参考前后的I帧或P帧来生成。B帧利用前后帧的信息来预测当前帧的内容，从而实现更高的压缩比。由于B帧的解码需要前后帧的信息，它不能独立解码，必须在解码序列中结合I帧和P帧来完成。<br>简单地讲，I 帧是一个完整的画面，而 P 帧和 B 帧记录的是相对于 I 帧的变化。如果没有 I 帧，P 帧和 B 帧就无法解码。</li>\n</ul>\n<p>在H.264压缩标准中I帧、P帧、B帧用于表示传输的视频画面。<br><img src=\"/2024/2f262a28/image-20240721152541316.png\"></p>\n<h2 id=\"GOP-和-IDR\"><a href=\"#GOP-和-IDR\" class=\"headerlink\" title=\"GOP 和 IDR\"></a>GOP 和 IDR</h2><p>在H264中图像以<strong>序列</strong>为单位进行组织，一个序列是一段图像编码后的数据流。<br>一个序列的第一个图像叫做 <strong>IDR 图像</strong>（<strong>立即刷新图像</strong>），IDR 图像都是 I 帧图像。H.264 引入 IDR 图像是为了解码的重同步，当解码器解码到 IDR 图像时，立即将参考帧队列清空，将已解码的数据全部输出或抛弃，重新查找参数集，开始一个新的序列。这样，如果前一个序列出现重大错误，在这里可以获得重新同步的机会。IDR图像之后的图像永远不会使用IDR之前的图像的数据来解码。<br>一个序列就是一段内容差异不太大的图像编码后生成的一串数据流。当运动变化比较少时，一个序列可以很长，因为运动变化少就代表图像画面的内容变动很小，所以就可以编一个I帧，然后一直P帧、B帧了。当运动变化多时，可能一个序列就比较短了，比如就包含一个I帧和3、4个P帧。<br>在视频编码序列中，GOP即Group of picture（<strong>图像组</strong>），指两个I帧之间的距离，Reference（<strong>参考周期</strong>）指两个P帧之间的距离。两个I帧之间形成一组图片，就是GOP（Group Of Picture）。</p>\n<p>【GOP示意图】<br><img src=\"/2024/2f262a28/image-20240721161215632.png\" alt=\"alt\"></p>\n<h2 id=\"I帧\"><a href=\"#I帧\" class=\"headerlink\" title=\"I帧\"></a>I帧</h2><p>I帧:即Intra-coded picture（<strong>帧内编码图像帧</strong>），I帧表示关键帧，你可以理解为这一帧画面的完整保留；解码时只需要本帧数据就可以完成（因为包含完整画面）。又称为内部画面 (intra picture)，I 帧通常是每个 GOP（MPEG 所使用的一种视频压缩技术）的第一个帧，经过适度地压缩，做为随机访问的参考点，可以当成图象。在MPEG编码的过程中，部分视频帧序列压缩成为I帧；部分压缩成P帧；还有部分压缩成B帧。I帧法是帧内压缩法，也称为“关键帧”压缩法。I帧法是基于离散余弦变换DCT（Discrete Cosine Transform）的压缩技术，这种算法与JPEG压缩算法类似。采用I帧压缩可达到1&#x2F;6的压缩比而无明显的压缩痕迹。</p>\n<p>【<strong>I帧特点</strong>】<br>　　1.它是一个全帧压缩编码帧。它将全帧图像信息进行JPEG压缩编码及传输;<br>　　2.解码时仅用I帧的数据就可重构完整图像;<br>　　3.I帧描述了图像背景和运动主体的详情;<br>　　4.I帧不需要参考其他画面而生成;<br>　　5.I帧是P帧和B帧的参考帧(其质量直接影响到同组中以后各帧的质量);<br>　　6.I帧是帧组GOP的基础帧(第一帧),在一组中只有一个I帧;<br>　　7.I帧不需要考虑运动矢量;<br>　　8.I帧所占数据的信息量比较大。</p>\n<p>【<strong>I帧编码流程</strong>】<br>　　(1)进行帧内预测，决定所采用的帧内预测模式。<br>　　(2)像素值减去预测值，得到残差。<br>　　(3)对残差进行变换和量化。<br>　　(4)变长编码和算术编码。<br>　　(5)重构图像并滤波，得到的图像作为其它帧的参考帧。</p>\n<p>例如：在视频会议系统中，终端发送给MCU（或者MCU发送给终端）的图像，并不是每次都把完整的一幅幅图片发送到远端，而只是发送后一幅画面在前一幅画面基础上发生变化的部分。如果在网络状况不好的情况下，终端的接收远端或者发送给远程的画面就会有丢包而出现图像花屏、图像卡顿的现象，在这种情况下如果没有I帧机制来让远端重新发一幅新的完整的图像到本地（或者本地重新发一幅新的完整的图像给远端），终端的输出图像的花屏、卡顿现象会越来越严重，从而造成会议无法正常进行。<br>在视频画面播放过程中，若I帧丢失了，则后面的P帧也就随着解不出来，就会出现视频画面黑屏的现象；若P帧丢失了，则视频画面会出现花屏、马赛克等现象。<br>在视频会议系统中I帧只会在会议限定的带宽内发生，不会超越会议带宽而生效。I帧机制不仅存在于MCU中，电视墙服务器、录播服务器中也存在。就是为了解决在网络状况不好的情况下，出现的丢包而造成的如图像花屏、卡顿，而影响会议会正常进行。</p>\n<h2 id=\"P-帧\"><a href=\"#P-帧\" class=\"headerlink\" title=\"P 帧\"></a>P 帧</h2><p>P帧:即Predictive-coded Picture（<strong>前向预测编码图像帧</strong>）。P帧表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。（也就是差别帧，P帧没有完整画面数据，只有与前一帧的画面差别的数据）</p>\n<p><img src=\"/2024/2f262a28/image-20240721162346288.png\"></p>\n<p>【<strong>P帧的预测与重构</strong>】<br>　　P帧是以I帧为参考帧,在I帧中找出P帧“某点”的预测值和运动矢量,取预测差值和运动矢量一起传送。在接收端根据运动矢量从I帧中找出P帧“某点”的预测值并与差值相加以得到P帧“某点”样值,从而可得到完整的P帧。</p>\n<p>【<strong>P帧特点</strong>】<br>　　1.P帧是I帧后面相隔1~2帧的编码帧;<br>　　2.P帧采用运动补偿的方法传送它与前面的I或P帧的差值及运动矢量(预测误差);<br>　　3.解码时必须将I帧中的预测值与预测误差求和后才能重构完整的P帧图像;<br>　　4.P帧属于前向预测的帧间编码。它只参考前面最靠近它的I帧或P帧;<br>　　5.P帧可以是其后面P帧的参考帧,也可以是其前后的B帧的参考帧;<br>　　6.由于P帧是参考帧,它可能造成解码错误的扩散;<br>　　7.由于是差值传送,P帧的压缩比较高。</p>\n<h2 id=\"B帧\"><a href=\"#B帧\" class=\"headerlink\" title=\"B帧\"></a>B帧</h2><p>B帧:即Bidirectionally predicted picture（<strong>双向预测编码图像帧</strong>)。B帧是双向差别帧，也就是B帧记录的是本帧与前后帧的差别，换言之，<strong>要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面</strong>，通过前后画面的与本帧数据的叠加取得最终的画面。B帧压缩率高，但是解码时CPU会比较累。<strong>无论是从编码侧还是解码侧看，B帧都是资源密集型</strong></p>\n<p><img src=\"/2024/2f262a28/image-20240721162537729.png\"></p>\n<p>【<strong>B帧的预测与重构</strong>】<br><strong>B帧以前面的I或P帧和后面的P帧为参考帧</strong>,“找出”B帧“某点”的预测值和两个运动矢量,并取预测差值和运动矢量传送。接收端根据运动矢量在两个参考帧中“找出(算出)”预测值并与差值求和,得到B帧“某点”样值,从而可得到完整的B帧。采用运动预测的方式进行帧间双向预测编码</p>\n<p>【<strong>B帧特点</strong>】<br>　　1.B帧是由前面的I或P帧和后面的P帧来进行预测的;<br>　　2.B帧传送的是<strong>它与前面的I帧或P帧和后面的P帧之间的预测误差及运动矢量</strong>;<br>　　3.B帧是双向预测编码帧;<br>　　4.B帧压缩比最高,因为它只反映丙参考帧间运动主体的变化情况,预测比较准确;<br>　　5.B帧不是参考帧,不会造成解码错误的扩散</p>\n<p>【<strong>为什么需要B帧</strong>】</p>\n<p> 从上面的看，我们知道I和P的解码算法比较简单，资源占用也比较少，I只要自己完成就行了，P呢，也只需要解码器把前一个画面缓存一下，遇到P时就使用之前缓存的画面就好了，如果视频流只有I和P，解码器可以不管后面的数据，边读边解码，线性前进，大家很舒服。那么为什么还要引入B帧？</p>\n<p>网络上的电影很多都采用了B帧，因为B帧记录的是前后帧的差别，<strong>比P帧能节约更多的空间</strong>，但这样一来，文件小了，解码器就麻烦了，因为在解码时，不仅要用之前缓存的画面，还要知道下一个I或者P的画面（也就是说要预读预解码），而且，B帧不能简单地丢掉，因为B帧其实也包含了画面信息，如果简单丢掉，并用之前的画面简单重复，就会造成画面卡（其实就是丢帧了），并且由于网络上的电影为了节约空间，往往使用相当多的B帧，B帧用的多，对不支持B帧的播放器就造成更大的困扰，画面也就越卡。</p>\n<p>【<strong>显示和解码顺序示意图</strong>】<br><img src=\"/2024/2f262a28/image-20240721162803162.png\"></p>\n<h2 id=\"PTS-DTS\"><a href=\"#PTS-DTS\" class=\"headerlink\" title=\"PTS DTS\"></a>PTS DTS</h2><p>【<strong>为什么会有PTS和DTS的概念</strong>】</p>\n<p>通过上面的描述可以看出：P帧需要参考前面的I帧或P帧才可以生成一张完整的图片，而B帧则需要参考前面I帧或P帧及其后面的一个P帧才可以生成一张完整的图片。这样就带来了一个问题：在视频流中，先到来的 B 帧无法立即解码，需要等待它依赖的后面的 I、P 帧先解码完成，这样一来播放时间与解码时间不一致了，顺序打乱了，那这些帧该如何播放呢？这时就引入了另外两个概念：DTS 和 PTS。</p>\n<p>【<strong>PTS和DTS</strong>】</p>\n<p>先来了解一下PTS和DTS的基本概念：</p>\n<p><strong>DTS（Decoding Time Stamp）</strong>：即<strong>解码时间戳</strong>，这个时间戳的意义在于告诉播放器该在什么时候解码这一帧的数据。<br><strong>PTS（Presentation Time Stamp）</strong>：即<strong>显示时间戳</strong>，这个时间戳用来告诉播放器该在什么时候显示这一帧的数据。</p>\n<p>虽然 DTS、PTS 是用于指导播放端的行为，但它们是在编码的时候由编码器生成的。</p>\n<p>在视频采集的时候是录制一帧就编码一帧发送一帧的，在编码的时候会生成 PTS，这里需要特别注意的是 frame（帧）的编码方式，在通常的场景中，编解码器编码一个 I 帧，然后向后跳过几个帧，用编码 I 帧作为基准帧对一个未来 P 帧进行编码，然后跳回到 I 帧之后的下一个帧。编码的 I 帧和 P 帧之间的帧被编码为 B 帧。之后，编码器会再次跳过几个帧，使用第一个 P 帧作为基准帧编码另外一个 P 帧，然后再次跳回，用 B 帧填充显示序列中的空隙。这个过程不断继续，每 12 到 15 个 P 帧和 B 帧内插入一个新的 I 帧。P 帧由前一个 I 帧或 P 帧图像来预测，而 B 帧由前后的两个 P 帧或一个 I 帧和一个 P 帧来预测，因而编解码和帧的显示顺序有所不同，如下所示：</p>\n<p><img src=\"/2024/2f262a28/image-20240721163414484.png\"></p>\n<p>假设编码器采集到的帧是这个样子的：<br><code> I B B P B B P </code></p>\n<p>那么它的显示顺序，也就是PTS应该是这样：<br><code> 1 2 3 4 5 6 7  </code></p>\n<p>编码器的编码顺序是：<br><code> 1 4 2 3 7 5 6 </code></p>\n<p>推流顺序也是按照编码顺序去推的，即<br><code> I P B B P B B </code></p>\n<p>那么接收断收到的视频流也就是<br><code> I P B B P B B </code></p>\n<p>这时候去解码，也是按照收到的视频流一帧一帧去解的了，接收一帧解码一帧，因为在编码的时候已经按照 I、B、P 的依赖关系编好了，接收到数据直接解码就好了。那么解码顺序是：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs text\">     I P B B P B B<br>DTS：1 2 3 4 5 6 7<br>PTS：1 4 2 3 7 5 6<br></code></pre></td></tr></table></figure>\n\n\n<p>可以看到解码出来对应的 PTS 不是顺序的，为了正确显示视频流，这时候我们就必须按照 PTS 重新调整解码后的 frame(帧)，即</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs text\">     I B B P B B P<br>DTS：1 3 4 2 6 7 5<br>PTS：1 2 3 4 5 6 7<br></code></pre></td></tr></table></figure>\n\n\n<p>另外，并不是一定要使用B帧。<strong>在实时互动直播系统中，很少使用B帧</strong>。主要的原因是压缩和解码B帧时，由于要双向参考，所以它需要缓冲更多的数据，且使用的CPU也会更高。由于实时性的要求，所以一般不使用它。不过对于播放器来说，遇到带有B帧的H264数据是常有的事儿。在没有B帧的情况下，存放帧的顺序和显示帧的顺序就是一样的，PTS和DTS的值也是一样的。</p>\n",
            "tags": [
                "audio",
                "video"
            ]
        }
    ]
}